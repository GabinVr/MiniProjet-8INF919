{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f21cd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.12.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer,\n",
    "    StopWordsRemover,\n",
    "    NGram,\n",
    "    HashingTF,\n",
    "    VectorAssembler,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Fonction 1 : nettoyage du texte\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def clean_text_column(df, text_col=\"text\", output_col=\"clean_text\"):\n",
    "    \"\"\"\n",
    "    Nettoie une colonne de texte :\n",
    "    - minuscules\n",
    "    - suppression URL\n",
    "    - suppression caractères non alphabétiques\n",
    "    - réduction espaces\n",
    "    \"\"\"\n",
    "\n",
    "    col = F.lower(F.col(text_col))\n",
    "    col = F.regexp_replace(col, r\"http\\S+|www\\.\\S+\", \" \")\n",
    "    col = F.regexp_replace(col, r\"[^a-z\\s]\", \" \")\n",
    "    col = F.regexp_replace(col, r\"\\s+\", \" \")\n",
    "    col = F.trim(col)\n",
    "\n",
    "    return df.withColumn(output_col, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f13d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(num_features=262144):\n",
    "    \"\"\"\n",
    "    Pipeline Spark :\n",
    "    1. Tokenizer\n",
    "    2. StopWordsRemover\n",
    "    3. NGram (2-gram)\n",
    "    4. HashingTF sur unigrams\n",
    "    5. HashingTF sur bigrams\n",
    "    6. VectorAssembler -> features\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = Tokenizer(\n",
    "        inputCol=\"clean_text\",\n",
    "        outputCol=\"tokens\"\n",
    "    )\n",
    "\n",
    "    remover = StopWordsRemover(\n",
    "        inputCol=\"tokens\",\n",
    "        outputCol=\"unigrams\"\n",
    "    )\n",
    "\n",
    "    bigrammer = NGram(\n",
    "        n=2,\n",
    "        inputCol=\"unigrams\",\n",
    "        outputCol=\"bigrams\"\n",
    "    )\n",
    "\n",
    "    hashing_unigrams = HashingTF(\n",
    "        numFeatures=num_features,\n",
    "        inputCol=\"unigrams\",\n",
    "        outputCol=\"unigram_features\"\n",
    "    )\n",
    "\n",
    "    hashing_bigrams = HashingTF(\n",
    "        numFeatures=num_features,\n",
    "        inputCol=\"bigrams\",\n",
    "        outputCol=\"bigram_features\"\n",
    "    )\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"unigram_features\", \"bigram_features\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[\n",
    "        tokenizer,\n",
    "        remover,\n",
    "        bigrammer,\n",
    "        hashing_unigrams,\n",
    "        hashing_bigrams,\n",
    "        assembler\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version de PySpark : 4.0.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"Version de PySpark :\", pyspark.__version__)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")  # ⚠️ TRÈS IMPORTANT : on force Spark à tourner en local\n",
    "    .appName(\"Test_Spark_Minimal\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "print(\"SparkContext master :\", spark.sparkContext.master)\n",
    "print(\"Spark version :\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99595af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"./train.csv\"   # à adapter\n",
    "path_output = \"./train_parquet_notebook\"   # dossier de sortie\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(path_train)\n",
    ")\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "\n",
    "print(\"✓ CSV chargé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ee6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\n",
    "    F.col(\"sentiment\").alias(\"label\"),\n",
    "    F.col(\"text\").cast(StringType())\n",
    ").dropna(subset=[\"label\", \"text\"])\n",
    "\n",
    "df.show(5)\n",
    "print(\"Nombre de lignes :\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd49e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = clean_text_column(df, text_col=\"text\", output_col=\"clean_text\")\n",
    "\n",
    "df_clean.select(\"text\", \"clean_text\").show(10, truncate=False)\n",
    "print(\"✓ Texte nettoyé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = build_pipeline(num_features=262144)\n",
    "print(\"✓ Pipeline créé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ad743",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"⏳ Entraînement du pipeline...\")\n",
    "model = pipeline.fit(df_clean)\n",
    "\n",
    "print(\"⏳ Transformation...\")\n",
    "df_final = model.transform(df_clean)\n",
    "\n",
    "df_final.select(\"label\", \"features\").show(5, truncate=False)\n",
    "\n",
    "print(\"✓ Transformation terminée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"⏳ Entraînement du pipeline...\")\n",
    "model = pipeline.fit(df_clean)\n",
    "\n",
    "print(\"⏳ Transformation...\")\n",
    "df_final = model.transform(df_clean)\n",
    "\n",
    "df_final.select(\"label\", \"features\").show(5, truncate=False)\n",
    "\n",
    "print(\"✓ Transformation terminée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23adea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"✓ Spark arrêté\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e2393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
