{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d585465",
   "metadata": {},
   "source": [
    "# Pré-traitement train/test (VS Code)\n",
    "\n",
    "Objectif : lire `src/data/train.csv` et `src/data/test.csv`, nettoyer la colonne `text`, puis écrire :\n",
    "- `src/data/train_clean.csv`\n",
    "- `src/data/test_clean.csv`\n",
    "\n",
    "Notes :\n",
    "- Ce notebook évite `google.colab.files.upload()` (non applicable sur VS Code).\n",
    "- L’écriture en **un seul fichier CSV** est gérée via `coalesce(1)` + renommage du `part-*.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyspark\n",
    "# Si tu es dans un venv/conda avec pyspark déjà installé, tu peux commenter la ligne ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67281ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer,\n",
    "    StopWordsRemover,\n",
    "    NGram,\n",
    "    HashingTF,\n",
    "    VectorAssembler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TextProcessingPipeline\")\n",
    "    .master(\"local[*]\")  # important pour Colab\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"✓ Spark démarré\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path().resolve()\n",
    "DATA_DIR = ROOT / \"src\" / \"data\"\n",
    "\n",
    "train_path = DATA_DIR / \"train.csv\"\n",
    "test_path  = DATA_DIR / \"test.csv\"\n",
    "\n",
    "assert train_path.exists(), f\"Fichier introuvable: {train_path}\"\n",
    "assert test_path.exists(), f\"Fichier introuvable: {test_path}\"\n",
    "\n",
    "train_df = spark.read.csv(str(train_path), header=True, inferSchema=True)\n",
    "test_df  = spark.read.csv(str(test_path), header=True, inferSchema=True)\n",
    "\n",
    "print(\"train cols:\", train_df.columns)\n",
    "print(\"test  cols:\", test_df.columns)\n",
    "print(\"train rows:\", train_df.count())\n",
    "print(\"test  rows:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def clean_text(df, text_col=\"text\"):\n",
    "    # Nettoyage minimal (adaptable)\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(text_col, F.lower(F.col(text_col)))\n",
    "        .withColumn(text_col, F.regexp_replace(F.col(text_col), r\"http\\S+|www\\.\\S+\", \" \"))      # URLs\n",
    "        .withColumn(text_col, F.regexp_replace(F.col(text_col), r\"[^a-z0-9\\s]\", \" \"))            # ponctuation/symboles\n",
    "        .withColumn(text_col, F.regexp_replace(F.col(text_col), r\"\\s+\", \" \"))                    # espaces multiples\n",
    "        .withColumn(text_col, F.trim(F.col(text_col)))\n",
    "    )\n",
    "\n",
    "# Vérifie la présence de la colonne texte\n",
    "for name, df_ in [(\"train\", train_df), (\"test\", test_df)]:\n",
    "    if \"text\" not in df_.columns:\n",
    "        raise ValueError(f\"Colonne 'text' absente dans {name}. Colonnes: {df_.columns}\")\n",
    "\n",
    "train_clean_df = clean_text(train_df, \"text\")\n",
    "test_clean_df  = clean_text(test_df, \"text\")\n",
    "\n",
    "train_clean_df.select(\"text\").show(3, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fe967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "\n",
    "def write_single_csv(df, out_csv_path: Path):\n",
    "    out_csv_path = Path(out_csv_path)\n",
    "    tmp_dir = out_csv_path.with_suffix(\"\")  # ex: train_clean (dossier)\n",
    "    if tmp_dir.exists():\n",
    "        shutil.rmtree(tmp_dir)\n",
    "    if out_csv_path.exists():\n",
    "        out_csv_path.unlink()\n",
    "\n",
    "    (\n",
    "        df.coalesce(1)\n",
    "          .write\n",
    "          .mode(\"overwrite\")\n",
    "          .option(\"header\", True)\n",
    "          .csv(str(tmp_dir))\n",
    "    )\n",
    "\n",
    "    part_files = glob.glob(str(tmp_dir / \"part-*.csv\"))\n",
    "    if not part_files:\n",
    "        raise RuntimeError(f\"Aucun part-*.csv trouvé dans {tmp_dir}\")\n",
    "\n",
    "    shutil.move(part_files[0], out_csv_path)\n",
    "    # Nettoie le dossier temporaire\n",
    "    shutil.rmtree(tmp_dir)\n",
    "\n",
    "    return out_csv_path\n",
    "\n",
    "out_train = write_single_csv(train_clean_df, DATA_DIR / \"train_clean.csv\")\n",
    "out_test  = write_single_csv(test_clean_df,  DATA_DIR / \"test_clean.csv\")\n",
    "\n",
    "print(\"✓ écrit:\", out_train)\n",
    "print(\"✓ écrit:\", out_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee38c2b",
   "metadata": {},
   "source": [
    "## Optionnel : featurisation (même pipeline appliqué à train + test)\n",
    "Si tu n’en as pas besoin pour ton rendu, tu peux ignorer cette section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca55bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "\n",
    "stopwords = StopWordsRemover(\n",
    "    inputCol=\"tokens\",\n",
    "    outputCol=\"filtered_tokens\"\n",
    ")\n",
    "\n",
    "ngram = NGram(\n",
    "    n=2,\n",
    "    inputCol=\"filtered_tokens\",\n",
    "    outputCol=\"ngrams\"\n",
    ")\n",
    "\n",
    "hashing_tf = HashingTF(\n",
    "    inputCol=\"ngrams\",\n",
    "    outputCol=\"features\",\n",
    "    numFeatures=2**18\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    stopwords,\n",
    "    ngram,\n",
    "    hashing_tf\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4072138",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entraînement du pipeline sur train...\")\n",
    "model = pipeline.fit(train_clean_df)\n",
    "\n",
    "print(\"Transformation train/test...\")\n",
    "train_feat = model.transform(train_clean_df)\n",
    "test_feat  = model.transform(test_clean_df)\n",
    "\n",
    "train_feat.select(\"features\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"✓ Spark arrêté\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}